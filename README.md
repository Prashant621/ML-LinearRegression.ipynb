# ğŸ  Housing Price Prediction using Linear Regression

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![NumPy](https://img.shields.io/badge/NumPy-Latest-013243.svg)](https://numpy.org/)
[![Pandas](https://img.shields.io/badge/Pandas-Latest-150458.svg)](https://pandas.pydata.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

A comprehensive implementation of **Simple and Multiple Linear Regression** from scratch using gradient descent to predict California housing prices. This project demonstrates fundamental machine learning concepts without relying on scikit-learn's built-in models.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Dataset](#dataset)
- [Installation](#installation)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Mathematical Foundation](#mathematical-foundation)
- [Results](#results)
- [Key Insights](#key-insights)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)

## ğŸ¯ Overview

This project implements linear regression algorithms from scratch to predict housing prices in California. It includes:

- **Task 1**: Simple Linear Regression using a single feature (housing median age)
- **Task 2**: Multiple Linear Regression using all available features

Both models are built using custom implementations of:
- Gradient Descent optimization
- Cost Function (Mean Squared Error)
- Feature Normalization

## âœ¨ Features

- ğŸ“Š **From-Scratch Implementation**: No use of sklearn's LinearRegression
- ğŸ”¢ **Gradient Descent**: Custom optimization algorithm
- ğŸ“ˆ **Feature Normalization**: Z-score standardization
- ğŸ¨ **Visualizations**: Clear plots showing model fit
- ğŸ“ **Detailed Documentation**: Comprehensive explanations of each step
- ğŸ§ª **Reproducible Results**: Consistent random seeds and parameters

## ğŸ“Š Dataset

The project uses the **California Housing Dataset**, which contains information about housing districts in California.

### Features:

| Feature | Description | Type |
|---------|-------------|------|
| `longitude` | Longitude coordinate | Continuous |
| `latitude` | Latitude coordinate | Continuous |
| `housing_median_age` | Median age of houses in the district | Continuous |
| `total_rooms` | Total number of rooms in the district | Continuous |
| `total_bedrooms` | Total number of bedrooms in the district | Continuous |
| `population` | Total population in the district | Continuous |
| `households` | Total number of households in the district | Continuous |
| `median_income` | Median income of households (in $10,000s) | Continuous |
| `ocean_proximity` | Proximity to ocean | Categorical |

### Target Variable:
- `median_house_value`: Median house value in the district (USD)

### Dataset Statistics:
- **Total Samples**: ~20,640 districts
- **Missing Values**: Present in `total_bedrooms` column
- **Data Type**: Numerical (continuous) + 1 categorical feature

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/housing-price-prediction.git
cd housing-price-prediction
```

2. **Create a virtual environment** (recommended)
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install required packages**
```bash
pip install -r requirements.txt
```

### Requirements

Create a `requirements.txt` file with:
```
numpy>=1.21.0
pandas>=1.3.0
matplotlib>=3.4.0
jupyter>=1.0.0
```

## ğŸ“ Project Structure

```
housing-price-prediction/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ housing.csv                 # California housing dataset
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ task1_simple_regression.ipynb    # Single feature regression
â”‚   â””â”€â”€ task2_multiple_regression.ipynb  # Multiple feature regression
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processing.py          # Data loading and cleaning
â”‚   â”œâ”€â”€ gradient_descent.py         # Optimization algorithms
â”‚   â”œâ”€â”€ cost_functions.py           # Loss calculation
â”‚   â””â”€â”€ visualization.py            # Plotting utilities
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ simple_regression_plot.png
â”‚   â””â”€â”€ coefficients.txt
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
```

## ğŸ’» Usage

### Quick Start

**Task 1: Simple Linear Regression**
```python
import pandas as pd
import numpy as np

# Load data
data = pd.read_csv('./housing.csv')

# Select single feature
X = data[['housing_median_age']].values.flatten()
Y = data['median_house_value'].values

# Add bias term
X_with_bias = np.column_stack((np.ones(len(X)), X))

# Run gradient descent
theta = gradient_descent(X_with_bias, Y, alpha=0.001, iterations=5000)

print(f"Intercept: {theta[0]:.2f}")
print(f"Coefficient: {theta[1]:.2f}")
```

**Task 2: Multiple Linear Regression**
```python
# Select multiple features
feature_columns = ['housing_median_age', 'total_rooms', 'total_bedrooms',
                   'population', 'households', 'median_income']
X = data[feature_columns].values
Y = data['median_house_value'].values

# Normalize features
X = (X - X.mean(axis=0)) / X.std(axis=0)

# Add bias term
X = np.column_stack((np.ones(X.shape[0]), X))

# Run gradient descent
theta = gradient_descent_multi(X, Y, iterations=10000, alpha=0.1)

print("Learned coefficients:", theta)
```

### Running the Jupyter Notebooks

```bash
jupyter notebook
# Navigate to notebooks/ and open task1_simple_regression.ipynb
```

## ğŸ§® Mathematical Foundation

### 1. Hypothesis Function

**Simple Linear Regression:**
```
h(x) = Î¸â‚€ + Î¸â‚x
```

**Multiple Linear Regression:**
```
h(x) = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
```

Or in matrix form:
```
h(X) = XÎ¸
```

### 2. Cost Function (Mean Squared Error)

```
J(Î¸) = (1/2m) Î£(h(xâ½â±â¾) - yâ½â±â¾)Â²
```

Where:
- `m` = number of training examples
- `h(xâ½â±â¾)` = predicted value
- `yâ½â±â¾` = actual value

### 3. Gradient Descent Update Rule

```
Î¸â±¼ := Î¸â±¼ - Î± Ã— (1/m) Î£(h(xâ½â±â¾) - yâ½â±â¾) Ã— xâ±¼â½â±â¾
```

Where:
- `Î±` = learning rate
- `Î¸â±¼` = parameter j
- `xâ±¼â½â±â¾` = feature j of example i

### 4. Feature Normalization (Z-score)

```
x_norm = (x - Î¼) / Ïƒ
```

Where:
- `Î¼` = mean of feature
- `Ïƒ` = standard deviation of feature

## ğŸ“ˆ Results

### Task 1: Simple Linear Regression

**Model Equation:**
```
Price = Î¸â‚€ + Î¸â‚ Ã— (Housing Median Age)
```

**Performance:**
- Uses only housing median age as predictor
- Provides baseline understanding of age-price relationship
- Limited predictive power due to single feature

**Interpretation:**
- **Intercept (Î¸â‚€)**: Expected price when age = 0 (baseline price)
- **Coefficient (Î¸â‚)**: Change in price per year increase in housing age

### Task 2: Multiple Linear Regression

**Model Equation:**
```
Price = Î¸â‚€ + Î¸â‚Ã—age + Î¸â‚‚Ã—rooms + Î¸â‚ƒÃ—bedrooms + Î¸â‚„Ã—population + Î¸â‚…Ã—households + Î¸â‚†Ã—income
```

**Performance:**
- Significantly better than single-feature model
- Captures complex relationships between multiple factors
- Each coefficient represents the marginal effect of that feature

**Interpretation:**
- **Each Î¸áµ¢**: Change in price for one unit increase in feature i (holding others constant)
- **Normalized features**: Coefficients are comparable in magnitude

## ğŸ’¡ Key Insights

### Model Comparison

| Aspect | Simple Regression | Multiple Regression |
|--------|------------------|---------------------|
| **Accuracy** | Lower | Higher âœ“ |
| **Interpretability** | Very Easy âœ“ | Moderate |
| **Visualization** | 2D Plot âœ“ | High-dimensional |
| **Features Used** | 1 | 6 |
| **Use Case** | Quick insights | Production models |

### Why Multiple Features Help

1. **Captures Complexity**: Real-world prices depend on many factors
2. **Reduces Bias**: Single feature may miss important predictors
3. **Better Generalization**: More information leads to better predictions
4. **Accounts for Interactions**: Multiple features can work together

### When to Use Each Model

**Simple Linear Regression:**
- âœ… Need quick, interpretable insights
- âœ… Exploring single-variable relationships
- âœ… Educational purposes
- âœ… Data visualization for presentations

**Multiple Linear Regression:**
- âœ… Production prediction systems
- âœ… Need high accuracy
- âœ… Sufficient data available
- âœ… Understanding complex relationships

## ğŸ”¬ Implementation Details

### Data Preprocessing

```python
# 1. Handle missing values
data_cleaned = data.dropna(subset=['housing_median_age', 'median_house_value'])

# 2. Feature normalization
X_normalized = (X - X.mean(axis=0)) / X.std(axis=0)

# 3. Add bias term
X_with_bias = np.column_stack((np.ones(X.shape[0]), X_normalized))
```

### Hyperparameters

| Task | Learning Rate (Î±) | Iterations | Features |
|------|------------------|------------|----------|
| Task 1 | 0.001 | 5,000 | 1 |
| Task 2 | 0.1 | 10,000 | 6 |

### Convergence Criteria

The algorithm stops when:
- Maximum iterations reached, OR
- Gradient norm < 1e-6 (convergence threshold)

## ğŸ“Š Visualizations

### Simple Linear Regression Plot

The scatter plot shows:
- **Red X marks**: Actual data points
- **Blue line**: Fitted regression line
- Clear trend visualization

### Feature Importance

In multiple regression, coefficients indicate:
- **Positive coefficient**: Feature increases price
- **Negative coefficient**: Feature decreases price
- **Magnitude**: Strength of relationship

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. **Fork the repository**
2. **Create a feature branch**
   ```bash
   git checkout -b feature/AmazingFeature
   ```
3. **Commit your changes**
   ```bash
   git commit -m 'Add some AmazingFeature'
   ```
4. **Push to the branch**
   ```bash
   git push origin feature/AmazingFeature
   ```
5. **Open a Pull Request**

### Areas for Contribution

- [ ] Add polynomial regression implementation
- [ ] Implement regularization (Ridge, Lasso)
- [ ] Add cross-validation
- [ ] Create interactive visualizations
- [ ] Add unit tests
- [ ] Improve documentation
- [ ] Add more evaluation metrics (RÂ², RMSE, MAE)

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Dataset**: California Housing Dataset (originally from StatLib repository)
- **Inspiration**: Andrew Ng's Machine Learning course
- **Mathematical Foundation**: Stanford CS229 course materials
- **Community**: Thanks to all contributors and users

## ğŸ“š Further Reading

### Recommended Resources

1. **Books**
   - "Pattern Recognition and Machine Learning" by Christopher Bishop
   - "The Elements of Statistical Learning" by Hastie, Tibshirani, and Friedman

2. **Courses**
   - [Andrew Ng's Machine Learning Course](https://www.coursera.org/learn/machine-learning)
   - [Stanford CS229: Machine Learning](http://cs229.stanford.edu/)

3. **Papers**
   - "Least Squares Optimization" - Classical papers on gradient descent
   - Feature normalization techniques in ML

### Related Projects

- [Scikit-learn LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- [TensorFlow Linear Regression](https://www.tensorflow.org/tutorials)

## ğŸ“§ Contact

**Project Maintainer**: Your Name
- GitHub: [@yourusername](https://github.com/yourusername)
- Email: your.email@example.com
- LinkedIn: [Your Profile](https://linkedin.com/in/yourprofile)

## ğŸŒŸ Show Your Support

If this project helped you learn or solve a problem, please consider:

- â­ **Starring** the repository
- ğŸ´ **Forking** for your own projects
- ğŸ“¢ **Sharing** with others who might find it useful
- ğŸ’¬ **Providing feedback** through issues

---

<div align="center">

**Made with â¤ï¸ and Python**

[Report Bug](https://github.com/yourusername/housing-price-prediction/issues) â€¢ 
[Request Feature](https://github.com/yourusername/housing-price-prediction/issues) â€¢ 
[Documentation](https://github.com/yourusername/housing-price-prediction/wiki)

</div>
